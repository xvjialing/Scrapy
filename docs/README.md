# Scrapy

## 正则表达式

| 字符              |                    作用                    |
| --------------- | :--------------------------------------: |
| ^               |                 匹配字符串的开头                 |
| $               |                匹配字符串的末尾。                 |
| .               | 匹配任意字符，除了换行符，当re.DOTALL标记被指定时，则可以匹配包括换行符的任意字符。 |
| [...]           |    用来表示一组字符,单独列出：[amk] 匹配 'a'，'m'或'k'    |
| [^...]          |     不在[]中的字符: \[^abc]匹配除了a,b,c之外的字符。     |
| re*             |               匹配0个或多个的表达式。               |
| re+             |            匹配1个或多个的表达式（至少一个）。            |
| re?             |    匹配0个或1个由前面的正则表达式定义的片段，非贪婪方式（从左往右）     |
| re{ n}          |               精确匹配n个前面的表达式               |
| re{ n, m}       |              匹配大于等于n个前面表达式               |
| re{ n, m}       |  匹配 n 到 m 次由前面的正则表达式定义的片段，贪婪方式（从右往左匹配）   |
| a\| b           |                  匹配a或b                   |
| \s              |        匹配任意空白字符 ，等价于 [\t\n\r\f].         |
| \S              |                 匹配任意非空字符                 |
| \w              |                  匹配字母数字                  |
| \W              |                 匹配非字母数字                  |
| [\u4E00-\u9FA5] |                  匹配任意汉字                  |
| \d              |            匹配任意数字，等价于 [0-9].             |
| \D              |                 匹配任意非数字                  |

## 网站树结构

![网站结构](../img/tree.jpg)

* 深度优先算法

深度优先算法的遍历顺序是A-B-D-E-I-C-F-G-H(递归实现)

* 广度优先算法

广度优先算法的遍历顺序是A-B-C-D-E-F-G-H-I(队列实现)

## 爬虫去重策略

1. 将访问过的url保存如数据库
2. 将访问过的url存入set中，只需要o(1)的代价就可以查询url。100000000\*2byte\*50个字符/1024/1024/1024=9G
3. url经过md5等方法哈希后保存入set中
4. 用bitmap方法，将访问过的url通过hash函数映射到某一位
5. bloomfilter方法对bitmap进行改进，多重hash函数降低冲突

## 字符串编码

1. 计算机只能处理数字，文本转换为数字才能处理。计算机中8个bit作为一个字节，所以一个字节能表示的最大数字是255。
2. 计算机是美国人发明的，一个字节就能表示所有字符了，所以ASCII(一个字节)编码就成为美国人的标准编码。
3. 但ASCII处理中文明显是不够的，中文不止255个汉字，所以制定了GB2312，用两个字节表示一个汉字。但不同国家发展的编码不同，标准不统一。
4. Unicode将所有的语言统一到一套编码中。但中英文都用Unicode浪费空间，所以出现了可变长的的编码"UTF-8",把英文变成一个字节，汉字变成3个字节，生僻字变成4-6个字节。



## 初始化工程

```
scrapy startproject articalScrapy
```

初始化的工程中，spiders文件夹用于存放爬虫文件，items.py用于定义数据保存的格式，middlewares用于保存中间件，pipeline.py用于数据存储操作，setting.py用于系统设置。

*创建爬虫模版*

```
scrapy genspider example example.com
```

运行单个爬虫

```
scrapy crawl jobbole
```

